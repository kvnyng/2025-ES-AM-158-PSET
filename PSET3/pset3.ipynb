{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3: DQN, Policy gradient and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due: 11:59pm, November 2, 2025**\n",
    "\n",
    "In the last PSET of reinforcement learning, you will implement the DQN algorithm together with vanilla policy gradient (REINFORCE) and its variants. In the last problem you will train a half-cheetah with stable baseline 3 package. \n",
    "\n",
    "- Problem 1 requires implementing DQN and double DQN. **TODOs:**\n",
    "  \n",
    "  - 1.1 Finish vanilla DQN (20 pt)\n",
    "  - 1.2 Finish double DQN (5 pt)\n",
    "\n",
    "- Problem 2 verify the policy gradient theory and requires implementing REINFORCE with learned value function. **TODOs:**\n",
    "  \n",
    "  - 2.1 Verify different policy gradient estimator (15 pt)\n",
    "  - 2.2 Implement REINFORCE with learned baseline (10 pt)\n",
    "\n",
    "- Problem 3 requires implementing vanilla on-policy actor-critic algorithm. **TODOs:**\n",
    "  - 3.1 Implement vanilla actor-critic (25 pt)\n",
    "\n",
    "All of these three algorithms works on `gym`'s `Acrobot-v1` environments.\n",
    "\n",
    "- Problem 4 requires implementing PPO algorithm. **TODOs:**\n",
    "  - 4.1 Implement PPO-cliping (25 pt)\n",
    "\n",
    "- Problem 5 **(Bonus)** help you try stable baseline 3 on `gym`'s `Half-cheetah-v4` environment. **TODOs:**\n",
    "  - 5.1 Tune the parameter in stable baseline 3 (**Note the training can take 15 min**) (20 pt)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement DQN on `Acrobot-v1` environment using `gym`.\n",
    "\n",
    "### Algorithm recap\n",
    "\n",
    "1. **Q-network**\n",
    "   - Function approximator $Q_\\theta(s,a)$ for action values (here: a small MLP).\n",
    "\n",
    "2. **Target network**\n",
    "   - A copy of the online network with parameters $\\theta^-$ that are updates periodically to stabilize training.\n",
    "\n",
    "3. **Experience replay**\n",
    "   - A replay buffer of transitions $(s,a,r,s',\\text{done})$. Sample i.i.d. minibatches to break temporal correlations.\n",
    "\n",
    "4. **Behavior policy**\n",
    "   - $\\epsilon$-greedy: with probability $\\epsilon$ choose a random action; otherwise choose $ \\arg\\max_a Q_\\theta(s,a) $.\n",
    "\n",
    "5. **TD targets**\n",
    "   - **Standard DQN**:\n",
    "     $$\n",
    "     y = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a')\n",
    "     $$\n",
    "   - **Double DQN**:\n",
    "     $$\n",
    "     a^\\star = \\arg\\max_{a'} Q_{\\theta}(s', a'), \\quad\n",
    "     y = r + \\gamma \\, Q_{\\theta^-}(s', a^\\star)\n",
    "     $$\n",
    "     “Online net selects, target net evaluates” reduces overestimation. In comparison to Double Q-learning, the weights of the second network $\\theta$ are replaced with the weights of the target network $\\theta^−$ for the evaluation of the current greedy policy. The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network.\n",
    "\n",
    "6. **Loss & optimization**\n",
    "   - Regress $Q_\\theta(s,a)$ to target $y$ using MSE loss; backpropagate to update $\\theta$.\n",
    "\n",
    "### Environment & action space\n",
    "\n",
    "- **Env**: `Acrobot-v1` (double pendulum swing-up) [Link](https://gymnasium.farama.org/environments/classic_control/acrobot/)\n",
    "- **Observation**: 6D —- $\\cos\\theta_1, \\sin\\theta_1, \\cos\\theta_2, \\sin\\theta_2, \\dot\\theta_1, \\dot\\theta_2$  \n",
    "- **Actions**: Discrete 3 actions —- torques $-1, 0, +1$  \n",
    "- **Reward**: $-1$ per step until the goal is reached (or the episode times out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implement DQN with gym\n",
    "\n",
    "**TODO:** Fill in the three TODO blocks.\n",
    "- implement a simple MLP\n",
    "- implement the replaybuffer class\n",
    "- implement the main algorithm\n",
    "\n",
    "All the given code is for reference. If you find it inconvenient feel free to write yourself.\n",
    "\n",
    "Note the final average return should be around $-100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Env] Acrobot-v1 | obs_dim=6, n_actions=3\n",
      "Episode 10 | Return: -500.00 | Avg Return: -500.00 | Avg Loss: 6.8593 | Eps: 0.951\n",
      "Episode 20 | Return: -353.00 | Avg Return: -485.30 | Avg Loss: 10.8519 | Eps: 0.905\n",
      "Episode 30 | Return: -500.00 | Avg Return: -484.20 | Avg Loss: 11.1172 | Eps: 0.860\n",
      "Episode 40 | Return: -348.00 | Avg Return: -429.10 | Avg Loss: 8.0195 | Eps: 0.818\n",
      "Episode 50 | Return: -295.00 | Avg Return: -394.30 | Avg Loss: 6.8804 | Eps: 0.778\n",
      "Episode 60 | Return: -279.00 | Avg Return: -366.60 | Avg Loss: 8.5754 | Eps: 0.740\n",
      "Episode 70 | Return: -255.00 | Avg Return: -319.00 | Avg Loss: 6.1911 | Eps: 0.704\n",
      "Episode 80 | Return: -329.00 | Avg Return: -271.00 | Avg Loss: 5.9328 | Eps: 0.670\n",
      "Episode 90 | Return: -346.00 | Avg Return: -256.80 | Avg Loss: 4.9844 | Eps: 0.637\n",
      "Episode 100 | Return: -287.00 | Avg Return: -233.60 | Avg Loss: 5.2454 | Eps: 0.606\n",
      "Episode 110 | Return: -205.00 | Avg Return: -246.60 | Avg Loss: 4.5546 | Eps: 0.576\n",
      "Episode 120 | Return: -151.00 | Avg Return: -191.70 | Avg Loss: 4.6856 | Eps: 0.548\n",
      "Episode 130 | Return: -246.00 | Avg Return: -217.20 | Avg Loss: 5.5593 | Eps: 0.521\n",
      "Episode 140 | Return: -140.00 | Avg Return: -178.60 | Avg Loss: 5.5495 | Eps: 0.496\n",
      "Episode 150 | Return: -155.00 | Avg Return: -164.80 | Avg Loss: 4.6765 | Eps: 0.471\n",
      "Episode 160 | Return: -144.00 | Avg Return: -163.30 | Avg Loss: 4.1319 | Eps: 0.448\n",
      "Episode 170 | Return: -153.00 | Avg Return: -143.70 | Avg Loss: 3.5612 | Eps: 0.427\n",
      "Episode 180 | Return: -162.00 | Avg Return: -156.20 | Avg Loss: 4.2096 | Eps: 0.406\n",
      "Episode 190 | Return: -181.00 | Avg Return: -137.60 | Avg Loss: 3.6723 | Eps: 0.386\n",
      "Episode 200 | Return: -173.00 | Avg Return: -154.60 | Avg Loss: 3.5151 | Eps: 0.367\n",
      "Episode 210 | Return: -131.00 | Avg Return: -132.70 | Avg Loss: 3.3529 | Eps: 0.349\n",
      "Episode 220 | Return: -119.00 | Avg Return: -134.10 | Avg Loss: 3.2302 | Eps: 0.332\n",
      "Episode 230 | Return: -131.00 | Avg Return: -118.40 | Avg Loss: 3.3358 | Eps: 0.316\n",
      "Episode 240 | Return: -120.00 | Avg Return: -108.70 | Avg Loss: 3.2023 | Eps: 0.300\n",
      "Episode 250 | Return: -107.00 | Avg Return: -110.90 | Avg Loss: 3.2163 | Eps: 0.286\n",
      "Episode 260 | Return: -123.00 | Avg Return: -113.10 | Avg Loss: 3.3085 | Eps: 0.272\n",
      "Episode 270 | Return: -104.00 | Avg Return: -111.90 | Avg Loss: 2.5797 | Eps: 0.258\n",
      "Episode 280 | Return: -91.00 | Avg Return: -102.00 | Avg Loss: 2.8703 | Eps: 0.246\n",
      "Episode 290 | Return: -84.00 | Avg Return: -117.40 | Avg Loss: 2.6367 | Eps: 0.234\n",
      "Episode 300 | Return: -101.00 | Avg Return: -104.90 | Avg Loss: 3.3747 | Eps: 0.222\n",
      "Episode 310 | Return: -97.00 | Avg Return: -112.90 | Avg Loss: 2.4585 | Eps: 0.211\n",
      "Episode 320 | Return: -108.00 | Avg Return: -104.20 | Avg Loss: 2.6161 | Eps: 0.201\n",
      "Episode 330 | Return: -132.00 | Avg Return: -107.70 | Avg Loss: 2.8414 | Eps: 0.191\n",
      "Episode 340 | Return: -87.00 | Avg Return: -104.20 | Avg Loss: 2.9110 | Eps: 0.182\n",
      "Episode 350 | Return: -118.00 | Avg Return: -116.50 | Avg Loss: 2.6779 | Eps: 0.173\n",
      "Episode 360 | Return: -108.00 | Avg Return: -105.20 | Avg Loss: 3.4378 | Eps: 0.165\n",
      "Episode 370 | Return: -128.00 | Avg Return: -101.60 | Avg Loss: 2.7024 | Eps: 0.157\n",
      "Episode 380 | Return: -94.00 | Avg Return: -104.20 | Avg Loss: 2.8653 | Eps: 0.149\n",
      "Episode 390 | Return: -124.00 | Avg Return: -99.50 | Avg Loss: 2.7454 | Eps: 0.142\n",
      "Episode 400 | Return: -104.00 | Avg Return: -101.70 | Avg Loss: 2.7379 | Eps: 0.135\n",
      "Episode 410 | Return: -76.00 | Avg Return: -98.60 | Avg Loss: 2.7094 | Eps: 0.128\n",
      "Episode 420 | Return: -95.00 | Avg Return: -88.20 | Avg Loss: 2.5464 | Eps: 0.122\n",
      "Episode 430 | Return: -98.00 | Avg Return: -97.40 | Avg Loss: 3.2747 | Eps: 0.116\n",
      "Episode 440 | Return: -84.00 | Avg Return: -95.30 | Avg Loss: 2.9048 | Eps: 0.110\n",
      "Episode 450 | Return: -104.00 | Avg Return: -132.20 | Avg Loss: 2.4769 | Eps: 0.105\n",
      "Episode 460 | Return: -93.00 | Avg Return: -91.70 | Avg Loss: 2.4084 | Eps: 0.100\n",
      "Episode 470 | Return: -98.00 | Avg Return: -95.90 | Avg Loss: 2.9487 | Eps: 0.095\n",
      "Episode 480 | Return: -91.00 | Avg Return: -101.90 | Avg Loss: 2.3229 | Eps: 0.090\n",
      "Episode 490 | Return: -97.00 | Avg Return: -94.90 | Avg Loss: 2.4031 | Eps: 0.086\n",
      "Episode 500 | Return: -92.00 | Avg Return: -89.10 | Avg Loss: 2.7703 | Eps: 0.082\n",
      "Episode 510 | Return: -122.00 | Avg Return: -97.70 | Avg Loss: 3.0607 | Eps: 0.078\n",
      "Episode 520 | Return: -94.00 | Avg Return: -90.10 | Avg Loss: 3.0879 | Eps: 0.074\n",
      "Episode 530 | Return: -119.00 | Avg Return: -89.70 | Avg Loss: 2.4818 | Eps: 0.070\n",
      "Episode 540 | Return: -97.00 | Avg Return: -88.90 | Avg Loss: 2.9550 | Eps: 0.067\n",
      "Episode 550 | Return: -93.00 | Avg Return: -88.80 | Avg Loss: 2.3866 | Eps: 0.063\n",
      "Episode 560 | Return: -100.00 | Avg Return: -75.80 | Avg Loss: 2.6738 | Eps: 0.060\n",
      "Episode 570 | Return: -78.00 | Avg Return: -82.40 | Avg Loss: 2.3370 | Eps: 0.057\n",
      "Episode 580 | Return: -118.00 | Avg Return: -84.10 | Avg Loss: 2.6174 | Eps: 0.055\n",
      "Episode 590 | Return: -77.00 | Avg Return: -84.10 | Avg Loss: 2.2518 | Eps: 0.052\n",
      "Episode 600 | Return: -109.00 | Avg Return: -92.40 | Avg Loss: 2.2240 | Eps: 0.049\n",
      "Episode 610 | Return: -103.00 | Avg Return: -100.90 | Avg Loss: 2.1063 | Eps: 0.047\n",
      "Episode 620 | Return: -83.00 | Avg Return: -89.60 | Avg Loss: 2.6722 | Eps: 0.045\n",
      "Episode 630 | Return: -87.00 | Avg Return: -81.30 | Avg Loss: 2.7023 | Eps: 0.043\n",
      "Episode 640 | Return: -82.00 | Avg Return: -93.10 | Avg Loss: 2.3633 | Eps: 0.040\n",
      "Episode 650 | Return: -74.00 | Avg Return: -77.50 | Avg Loss: 2.1908 | Eps: 0.038\n",
      "Episode 660 | Return: -83.00 | Avg Return: -86.90 | Avg Loss: 2.6300 | Eps: 0.037\n",
      "Episode 670 | Return: -85.00 | Avg Return: -89.40 | Avg Loss: 2.9693 | Eps: 0.035\n",
      "Episode 680 | Return: -111.00 | Avg Return: -93.00 | Avg Loss: 2.3561 | Eps: 0.033\n",
      "Episode 690 | Return: -75.00 | Avg Return: -79.40 | Avg Loss: 2.6185 | Eps: 0.031\n",
      "Episode 700 | Return: -91.00 | Avg Return: -85.00 | Avg Loss: 2.8565 | Eps: 0.030\n",
      "Episode 710 | Return: -70.00 | Avg Return: -92.80 | Avg Loss: 2.8791 | Eps: 0.028\n",
      "Episode 720 | Return: -84.00 | Avg Return: -94.00 | Avg Loss: 2.1930 | Eps: 0.027\n",
      "Episode 730 | Return: -85.00 | Avg Return: -82.30 | Avg Loss: 1.7054 | Eps: 0.026\n",
      "Episode 740 | Return: -75.00 | Avg Return: -84.80 | Avg Loss: 2.5486 | Eps: 0.024\n",
      "Episode 750 | Return: -87.00 | Avg Return: -87.40 | Avg Loss: 2.4749 | Eps: 0.023\n",
      "Episode 760 | Return: -79.00 | Avg Return: -103.50 | Avg Loss: 2.7482 | Eps: 0.022\n",
      "Episode 770 | Return: -73.00 | Avg Return: -79.60 | Avg Loss: 1.9481 | Eps: 0.021\n",
      "Episode 780 | Return: -71.00 | Avg Return: -88.50 | Avg Loss: 1.9443 | Eps: 0.020\n",
      "Episode 790 | Return: -81.00 | Avg Return: -80.00 | Avg Loss: 2.1646 | Eps: 0.019\n",
      "Episode 800 | Return: -80.00 | Avg Return: -81.80 | Avg Loss: 2.3513 | Eps: 0.018\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os, random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# ------------------- config  -------------------\n",
    "ENV_ID                 = \"Acrobot-v1\"\n",
    "SEED                   = 42\n",
    "GAMMA                  = 0.995\n",
    "LR                     = 1e-3\n",
    "BATCH_SIZE             = 64\n",
    "BUFFER_SIZE            = 100_000\n",
    "START_TRAINING_AFTER   = 1000       # warmup steps\n",
    "TARGET_UPDATE_FREQ     = 10       # steps (hard update)\n",
    "MAX_EPISODES           = 800\n",
    "\n",
    "GRAD_CLIP_NORM         = 10.0\n",
    "PRINT_EVERY_EPISODES   = 10\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "state_dims = int(np.prod(env.observation_space.shape))  # Acrobot: 6\n",
    "num_actions = env.action_space.n                        # Acrobot: 3\n",
    "print(f\"[Env] {ENV_ID} | obs_dim={state_dims}, n_actions={num_actions}\")\n",
    "\n",
    "# ------------------- gym environment -> tensor -------------------\n",
    "class TWrapper(gym.Wrapper):\n",
    "    def __init__(self, env): super().__init__(env)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        return torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0), info\n",
    "    def step(self, action):\n",
    "        a = int(action.item()) if isinstance(action, torch.Tensor) else int(action)\n",
    "        obs, r, term, trunc, info = self.env.step(a)\n",
    "        done = bool(term or trunc)\n",
    "        obs_t = torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0)\n",
    "        r_t   = torch.tensor([[r]], dtype=torch.float32)\n",
    "        d_t   = torch.tensor([[done]], dtype=torch.bool)\n",
    "        return obs_t, r_t, d_t, info\n",
    "\n",
    "env = TWrapper(env)\n",
    "\n",
    "# ------------------- Q network -------------------\n",
    "class QNetwork(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 1.1: Implement a simple MLP\n",
    "    #########################################\n",
    "    def __init__(self, state_dim, num_actions, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "q_net = QNetwork(state_dims, num_actions)\n",
    "tgt_net = copy.deepcopy(q_net).eval()\n",
    "\n",
    "# ------------------- simple replay buffer -------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=BUFFER_SIZE):\n",
    "        self.capacity, self.mem, self.pos = capacity, [], 0\n",
    "    #########################################\n",
    "    #TODO 1.1: Implement a ReplayBuffer\n",
    "    # capacity: max number of transitions to store\n",
    "    # mem: list of transitions\n",
    "    # pos: next position to insert\n",
    "    # push: add a transition\n",
    "    # sample: random sample a batch of transitions\n",
    "    #########################################\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Store tensors - we'll detach them when sampling to avoid keeping computation graphs\n",
    "        # But for now, store them as-is so they can participate in future computation graphs\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        if len(self.mem) < self.capacity:\n",
    "            self.mem.append(transition)\n",
    "        else:\n",
    "            self.mem[self.pos] = transition\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.mem, batch_size)\n",
    "        # Concatenate tensors\n",
    "        # Note: If tensors were detached when stored, they're already detached\n",
    "        # When we use detached tensors as input to networks, gradients still flow through network parameters\n",
    "        states = torch.cat([t[0] for t in batch], dim=0)\n",
    "        actions = torch.cat([t[1] for t in batch], dim=0)\n",
    "        rewards = torch.cat([t[2] for t in batch], dim=0)\n",
    "        next_states = torch.cat([t[3] for t in batch], dim=0)\n",
    "        dones = torch.cat([t[4] for t in batch], dim=0)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mem)\n",
    "    \n",
    "buffer = ReplayBuffer()\n",
    "optim  = Adam(q_net.parameters(), lr=LR)\n",
    "\n",
    "# ------------------- greedy / epsilon-greedy -------------------\n",
    "@torch.no_grad()\n",
    "def act_epsilon_greedy(state: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "    if torch.rand(1).item() < eps:\n",
    "        return torch.randint(num_actions, (1, 1))\n",
    "    q = q_net(state)\n",
    "    return torch.argmax(q, dim=-1, keepdim=True)\n",
    "\n",
    "# ------------------- train loop (Double DQN target) -------------------\n",
    "def train():\n",
    "    # Ensure gradients are enabled globally\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    q_net.train()  # Ensure network is in training mode\n",
    "    # Verify all parameters require gradients\n",
    "    for name, param in q_net.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"Warning: {name} does not require grad! Enabling gradients.\")\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    returns_hist, loss_hist = [], []\n",
    "    eps = 1.0\n",
    "    eps_decay = 0.995\n",
    "    eps_min = 0.01\n",
    "    step_count = 0\n",
    "\n",
    "    for ep in range(1, MAX_EPISODES + 1):\n",
    "        #########################################\n",
    "        #TODO 1.1: Implement the main algorithm here\n",
    "        #########################################\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0.0\n",
    "        ep_losses = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            action = act_epsilon_greedy(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            ep_return += reward.item()\n",
    "            step_count += 1\n",
    "            \n",
    "            # Training step\n",
    "            if len(buffer) >= START_TRAINING_AFTER and step_count % 1 == 0:\n",
    "                # Sample batch\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(BATCH_SIZE)\n",
    "                \n",
    "                # Create completely fresh tensors from the data to ensure clean computation graph\n",
    "                # Convert to numpy first, then back to tensor - this creates truly fresh leaf tensors\n",
    "                states_np = states.detach().cpu().numpy()\n",
    "                states = torch.tensor(states_np, dtype=states.dtype, device=states.device)\n",
    "                \n",
    "                next_states_np = next_states.detach().cpu().numpy()\n",
    "                next_states = torch.tensor(next_states_np, dtype=next_states.dtype, device=next_states.device)\n",
    "                \n",
    "                # Verify network parameters require grad\n",
    "                if not any(p.requires_grad for p in q_net.parameters()):\n",
    "                    raise RuntimeError(\"No network parameters require grad!\")\n",
    "                \n",
    "                # Compute Q-values for current states\n",
    "                q_values = q_net(states)\n",
    "                \n",
    "                # Ensure actions are long integers for gather\n",
    "                actions_long = actions.long() if actions.dtype != torch.long else actions\n",
    "                q_value = q_values.gather(1, actions_long)\n",
    "                \n",
    "                # Verify q_value requires grad\n",
    "                if not q_value.requires_grad:\n",
    "                    raise RuntimeError(\"q_value should require grad for backpropagation\")\n",
    "                \n",
    "                # Compute targets (no gradients needed)\n",
    "                with torch.no_grad():\n",
    "                    #########################################\n",
    "                    #TODO 1.2: Change from DQN to Double DQN\n",
    "                    #########################################\n",
    "                    # Standard DQN (commented out):\n",
    "                    next_q_values = tgt_net(next_states)\n",
    "                    next_q_value = next_q_values.max(1, keepdim=True)[0]\n",
    "                    \n",
    "                    # Double DQN: online net selects, target net evaluates\n",
    "                    # next_q_values_online = q_net(next_states)\n",
    "                    # next_actions = next_q_values_online.argmax(1, keepdim=True)\n",
    "                    # next_q_values_target = tgt_net(next_states)\n",
    "                    # next_q_value = next_q_values_target.gather(1, next_actions)\n",
    "                    \n",
    "                    target = rewards + (GAMMA * next_q_value * (~dones))\n",
    "                \n",
    "                # Ensure target doesn't require grad (safeguard)\n",
    "                target = target.detach()\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(q_value, target)\n",
    "                \n",
    "                # Verify loss requires grad\n",
    "                if not loss.requires_grad:\n",
    "                    raise RuntimeError(\"loss should require grad for backpropagation\")\n",
    "                ep_losses.append(loss.item())\n",
    "                \n",
    "                # Optimize\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(q_net.parameters(), GRAD_CLIP_NORM)\n",
    "                optim.step()\n",
    "                \n",
    "                # Update target network\n",
    "                if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                    tgt_net.load_state_dict(q_net.state_dict())\n",
    "        \n",
    "        # Update epsilon\n",
    "        eps = max(eps_min, eps * eps_decay)\n",
    "        \n",
    "        # Record statistics\n",
    "        returns_hist.append(ep_return)\n",
    "        if ep_losses:\n",
    "            loss_hist.append(np.mean(ep_losses))\n",
    "        else:\n",
    "            loss_hist.append(0.0)\n",
    "        \n",
    "        # Print progress\n",
    "        if ep % PRINT_EVERY_EPISODES == 0:\n",
    "            avg_return = np.mean(returns_hist[-PRINT_EVERY_EPISODES:])\n",
    "            avg_loss = np.mean(loss_hist[-PRINT_EVERY_EPISODES:]) if loss_hist[-PRINT_EVERY_EPISODES:] else 0.0\n",
    "            print(f\"Episode {ep} | Return: {ep_return:.2f} | Avg Return: {avg_return:.2f} | Avg Loss: {avg_loss:.4f} | Eps: {eps:.3f}\")\n",
    "        \n",
    "    plot_stats({\"Returns\": returns_hist, \"Loss\": loss_hist})\n",
    "\n",
    "# ------------------- plotting -------------------\n",
    "def _smooth(x, w=21):\n",
    "    if len(x) < w: return x\n",
    "    k = w // 2\n",
    "    return [np.mean(x[max(0, i-k):min(len(x), i+k+1)]) for i in range(len(x))]\n",
    "\n",
    "def plot_stats(stats: dict, win: int = 21):\n",
    "    fig, axs = plt.subplots(len(stats), 1, figsize=(9, 5), tight_layout=True)\n",
    "    if len(stats) == 1: axs = [axs]\n",
    "    for ax, (k, v) in zip(axs, stats.items()):\n",
    "        ax.plot(_smooth(v, win))\n",
    "        ax.set_title(k)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "train()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Change classical DQN to double DQN\n",
    "\n",
    "Use **two networks**:\n",
    "- **Online** network selects the next action  \n",
    "  $$\n",
    "  a^* = \\arg\\max_{a'} Q_{\\text{online}}(s', a').\n",
    "  $$\n",
    "- **Target** network evaluates that action  \n",
    "  $$\n",
    "  y_{\\text{DDQN}} = r + \\gamma\\, Q_{\\text{target}}(s', a^*).\n",
    "  $$\n",
    "  \n",
    "This decoupling reduces overestimation while keeping the update otherwise unchanged.\n",
    "\n",
    "In the code you will only need to change several lines. \n",
    "\n",
    "**TODO:** Comment the vanilla DQN and write Double DQN at the same place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### reparameterization trick:\n",
    "\n",
    "Let $x$ be a random variable whose distribution depends on $\\theta$. Write this as $x\\sim p_\\theta(x)$. For any measurable $V:\\mathcal X\\to\\mathbb R$,\n",
    "$$\n",
    "\\mathbb{E}\\big[V(f(\\theta))\\big] \\;=\\; \\mathbb{E}_{x\\sim p_\\theta}[V(x)],\n",
    "$$\n",
    "where $p_\\theta$ is the distribution of $x$ induced by $\\theta$. This help us move complex functions from expectation to distribution.\n",
    "\n",
    "Take the gradient gives us\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbb{E}_{x\\sim p_\\theta}[V(x)] = \\int \\nabla p_\\theta(x) V(x) dx = \\int \\nabla \\ln(p_\\theta(x)) V(x) p_\\theta(x)dx = \\mathbb{E}_{x\\sim p_\\theta}[V(x) \\nabla \\ln(p_\\theta(x))]\n",
    "$$\n",
    "\n",
    "So now we only need to take derivative of the distribution.\n",
    "\n",
    "### RL as an Expectation over Trajectories\n",
    "The cost over a distribution of $s_0$ gives the definition of reward function $J$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s_0\\sim\\rho}[V^{\\pi_\\theta}(s_0)]\n",
    "$$\n",
    "\n",
    "But use the trick above we can rewrite that using trajectory. A trajectory is\n",
    "$$\n",
    "\\tau=(s_0,a_0,s_1,a_1,\\ldots,s_T),\n",
    "$$\n",
    "generated by initial state distribution $\\rho$, policy $\\pi_\\theta(a\\mid s)$, and dynamics $P(s'\\mid s,a)$. The trajectory distribution is\n",
    "$$\n",
    "p_\\theta(\\tau)=\\rho(s_0)\\prod_{t=0}^{T-1}\\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t).\n",
    "$$\n",
    "Define the discounted return\n",
    "$$\n",
    "G(\\tau)=\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t).\n",
    "$$\n",
    "Then the performance objective is\n",
    "$$\n",
    "\\,J(\\theta)=\\mathbb{E}_{\\tau\\sim p_\\theta}[G(\\tau)]\\,\n",
    "$$\n",
    "\n",
    "This helps move the $\\theta$ to distribution.\n",
    " -->\n",
    "\n",
    "### Recall: Policy-Gradient Theorem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\nabla_\\theta \\mathbb{E}_{\\tau\\sim p_\\theta}[G(\\tau)]\n",
    "= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[G(\\tau)\\,\\nabla_\\theta\\log p_\\theta(\\tau)\\right] \\\\\n",
    "&= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G(\\tau)\\right] \\\\\n",
    "\\end{aligned} \\tag{F1}\n",
    "$$\n",
    "\n",
    "This is the first gradient formulation we arrive at (here $G(\\tau) = R(\\tau)$ and $R(\\tau)$ is the notation used in Lecture notes). A naive collary is the using causality to change that to return-to-go:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t(\\tau)\\right]\n",
    "\\end{aligned} \\tag{F2} \n",
    "$$\n",
    "\n",
    "where $G_t=\\sum_{k=t}^{T-1}\\gamma^{k-t}r(s_k,a_k)$ and $d^{\\pi_\\theta}$ is the discounted state-visitation distribution. Next, we observe that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{a_t\\sim\\pi_\\theta}\\!\\left[\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0,\n",
    "$$\n",
    "\n",
    "<!-- This is because condition on $s_t$, $b(s_t)$ is a constant inside expectation, and $\\mathbb{E}_{a_t\\sim\\pi_\\theta}\\!\\left[\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\right]=0$.  -->\n",
    "\n",
    "Plug in the \"baseline\" $b(s_t)$ into the policy gradient gives us\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,(G_t(\\tau)- b(s_t))\\right] \\tag{F3}\n",
    "$$\n",
    "\n",
    "In practice most of the time people use the learned value function for the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient theorem (extended)\n",
    "\n",
    "Next we talk about another 3 chosen of policy gradient:\n",
    "\n",
    "From (F2) gradient we can easily see that $Q(s_t,a_t) = \\mathbb{E}[G_t(\\tau)]$, so plug in F2 gives us\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,Q(s_t,a_t)\\right] \\tag{F4}\n",
    "$$\n",
    "\n",
    "And followed by previous explanation of baseline, we can define $A(s_t,a_t) = Q(s_t,a_t) - V(s_t)$, thus we arrive the *advantage function* gradient.\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,A(s_t,a_t)\\right] \\tag{F5}\n",
    "$$\n",
    "\n",
    "The last formulation is by observing that\n",
    "$$\n",
    "\\mathbb{E}[Q(s_t,a_t)] = \\mathbb{E}[r(s_t,a_t) + \\gamma V(s_{t+1})]\n",
    "$$\n",
    "\n",
    "apply baseline to it gives us\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,(r(s_t,a_t) + \\gamma V(s_{t+1}) - V(s_t))\\right] \\tag{F6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convergence of Different Policy-Gradient Estimators\n",
    "\n",
    "We study a random walk on a ring. Let $\\mathcal{S}=\\mathbb{Z}_N$ and $\\mathcal{A}=\\{L,R\\}$. The dynamics and rewards are\n",
    "$$\n",
    "s_{t+1}\\equiv s_t+\\begin{cases}\n",
    "+1 & \\text{if }a_t = R, \\\\\n",
    "-1 & \\text{if }a_t = L,\n",
    "\\end{cases}\\ (\\mathrm{mod}\\,N),\\qquad\n",
    "r_t \\equiv \\begin{cases}\n",
    "r_\\text{terminal} & \\text{if done} \\\\\n",
    "r_\\text{step} & \\text{otherwise}\n",
    "\\end{cases},\\quad \\gamma\\in(0,1).\n",
    "$$\n",
    "\n",
    "Because this is tabular, Bellman consistency (Eq. 1.21) yields a linear system $AV=b$ (as in PSET1, Problem 4). Solving gives the exact values $V$, and $Q$ follows by one-step lookahead.\n",
    "\n",
    "**TODO:**\n",
    "1. Implement a minimal MLP policy ($x=s/N\\to\\pi_\\theta(\\cdot\\mid s)$) with a Softmax output.  \n",
    "2. Implement six MC gradient estimators: REINFORCE, return-to-go, baseline with $V$, using $Q$, advantage $Q{-}V$, and TD-residual.  \n",
    "3. Plot per-parameter sample std and the running-mean error $\\|\\bar g_k-\\nabla_\\theta J\\|_2$ vs. episodes, plus $|\\bar J_k-J_{\\text{true}}|$.\n",
    "   $$\\bar g_k = \\frac{1}{k} \\sum_i^k g_i, \\quad \\bar J_k = \\frac{1}{k} \\sum_i^k J_i$$\n",
    "   \n",
    "4. Comment on what you see, and explain it intuitively.\n",
    "\n",
    "Note:\n",
    "\n",
    "Here we provide the function `build_system`, `get_V_and_J` and `get_Q` for calculate the true value / action value. `finite_difference_grad` for approximate the true objective / gradient by finite difference. And also `logp_single` and `score_matrix_batch` for calculate $\\nabla J_\\theta$ in a batched manner (You can also use for-loop, but that takes quite long run time). But feel free to use your own code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Problem 2.1: Policy Gradient Estimators\n",
      "============================================================\n",
      "[main] Using device: cpu\n",
      "[main] Environment: N=10, gamma=0.9, running_reward=-1.0, terminal_reward=100.0\n",
      "[main] Creating PolicyNet with hidden=2...\n",
      "[main] Model created with 16 total parameters\n",
      "[main] Model device: cpu\n",
      "[main] Computing exact J and finite-difference gradient...\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[PolicyNet.forward] Input x shape: torch.Size([9, 1])\n",
      "[PolicyNet.forward] fc1 weight shape: torch.Size([2, 1]), bias shape: torch.Size([2])\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[main] J_true = 35.173882, grad_fd shape = torch.Size([16]), grad_fd norm = 19.759146\n",
      "[main] Computing V and Q from dynamic programming...\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[main] V shape: torch.Size([11]), V range: [0.000, 76.176]\n",
      "[build_system] N=10, device=cpu\n",
      "[build_system] s_idx shape: torch.Size([9])\n",
      "[build_system] x (before reshape) shape: torch.Size([9])\n",
      "[build_system] x (after reshape) shape: torch.Size([9, 1])\n",
      "[build_system] First layer input dim: 1, output dim: 2\n",
      "[build_system] pi shape: torch.Size([9, 2])\n",
      "[build_system] p shape: (9,), mean p(right): 0.578\n",
      "[main] Q shape: torch.Size([11, 2]), Q range: [0.000, 100.000]\n",
      "[main] Starting Monte Carlo gradient estimation with 20000 episodes...\n",
      "[main] Initial state batch shape: torch.Size([20000]), range: [1, 9]\n",
      "[mc_grad_estimators] Starting with N=10, gamma=0.9\n",
      "[mc_grad_estimators] device=cpu\n",
      "[mc_grad_estimators] Number of parameters P=16\n",
      "[mc_grad_estimators] Batch size B=20000\n",
      "[mc_grad_estimators] Initial states shape: torch.Size([20000]), initial done count: 0\n",
      "[mc_grad_estimators] V provided, shape: torch.Size([11])\n",
      "[mc_grad_estimators] Q provided, shape: torch.Size([11, 2])\n",
      "[mc_grad_estimators] Loop finished after 183 steps\n",
      "[mc_grad_estimators] Total trajectory steps recorded: 183\n",
      "[mc_grad_estimators] Processing 20000 episodes for return-to-go computation...\n",
      "[mc_grad_estimators] Processed 5000/20000 episodes...\n",
      "[mc_grad_estimators] Processed 10000/20000 episodes...\n",
      "[mc_grad_estimators] Processed 15000/20000 episodes...\n",
      "[mc_grad_estimators] Processed 20000/20000 episodes...\n",
      "[mc_grad_estimators] Finished processing all 20000 episodes\n",
      "[mc_grad_estimators] Computing g1 from J and H...\n",
      "[mc_grad_estimators] J shape: torch.Size([20000]), H shape: torch.Size([20000, 16])\n",
      "[mc_grad_estimators] Tensors are already detached from computation\n",
      "[mc_grad_estimators] Clearing H tensor...\n",
      "[mc_grad_estimators] H cleared\n",
      "[mc_grad_estimators] Printing final statistics...\n",
      "[mc_grad_estimators] Final shapes: g1=torch.Size([20000, 16]), g2=torch.Size([20000, 16]), J=torch.Size([20000])\n",
      "[mc_grad_estimators] g1.requires_grad=False, g2.requires_grad=False\n",
      "[mc_grad_estimators] J mean: 34.901, std: 37.365\n",
      "[mc_grad_estimators] Clearing trajectory data to free memory...\n",
      "[mc_grad_estimators] Trajectory lists length: rewards=183, scores=183\n",
      "[mc_grad_estimators] Trajectory data cleared successfully\n",
      "[mc_grad_estimators] Memory usage before conversion: 413.47 MB\n",
      "[mc_grad_estimators] Converting g1 to numpy...\n",
      "[mc_grad_estimators] g1 requires_grad: False, device: cpu\n",
      "[mc_grad_estimators] g1_np shape: (20000, 16), dtype: float32, size: 1.22 MB\n",
      "[mc_grad_estimators] g1 conversion successful\n",
      "[mc_grad_estimators] Converting g2 to numpy...\n",
      "[mc_grad_estimators] g2_np shape: (20000, 16), dtype: float32, size: 1.22 MB\n",
      "[mc_grad_estimators] Converting g3 to numpy...\n",
      "[mc_grad_estimators] g3_np shape: (20000, 16)\n",
      "[mc_grad_estimators] Converting g4 to numpy...\n",
      "[mc_grad_estimators] g4_np shape: (20000, 16)\n",
      "[mc_grad_estimators] Converting g5 to numpy...\n",
      "[mc_grad_estimators] g5_np shape: (20000, 16)\n",
      "[mc_grad_estimators] Converting g6 to numpy...\n",
      "[mc_grad_estimators] g6_np shape: (20000, 16)\n",
      "[mc_grad_estimators] Converting J to numpy...\n",
      "[mc_grad_estimators] Error converting J: cannot access local variable 'H' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/yc/qgv_p3r962b0jxw6kjv5k9m40000gn/T/ipykernel_73456/4094057281.py\", line 553, in mc_grad_estimators\n",
      "    del J, H\n",
      "           ^\n",
      "UnboundLocalError: cannot access local variable 'H' where it is not associated with a value\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'H' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 615\u001b[39m\n\u001b[32m    612\u001b[39m s0_batch = torch.randint(\u001b[32m1\u001b[39m, N, (episodes,), dtype=torch.int64, device=device)\n\u001b[32m    613\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[main] Initial state batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms0_batch.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms0_batch.min().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms0_batch.max().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m g1, g2, g3, g4, g5, g6, J = \u001b[43mmc_grad_estimators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_cost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunning_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mterminal_reward\u001b[49m\u001b[43m=\u001b[49m\u001b[43mterminal_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[43m=\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQ\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[main] Monte Carlo estimation completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    623\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[main] Returned shapes: g1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg1.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, g2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg2.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, g3=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg3.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 553\u001b[39m, in \u001b[36mmc_grad_estimators\u001b[39m\u001b[34m(model, N, s0_batch, gamma, step_cost, terminal_reward, V, Q)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[mc_grad_estimators] Converting J to numpy...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    552\u001b[39m J_np = J.detach().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m J, \u001b[43mH\u001b[49m\n\u001b[32m    554\u001b[39m gc.collect()\n\u001b[32m    555\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[mc_grad_estimators] J_np shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJ_np.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJ_np.nbytes\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1024\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m KB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'H' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import gc\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.func import functional_call, vmap, jacrev\n",
    "\n",
    "\n",
    "# ----------------- utilities -----------------\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# ----------------- policy network (last layer Softmax) -----------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.1: Implement the policy network\n",
    "    #########################################\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        # Input: normalized state s/N (scalar, shape: [B, 1])\n",
    "        # Output: logits for 2 actions (Left, Right), then Softmax\n",
    "        self.fc1 = nn.Linear(1, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 2)  # 2 actions: Left (0), Right (1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, 1] normalized state\n",
    "        if not hasattr(self, '_debug_printed'):\n",
    "            print(f\"[PolicyNet.forward] Input x shape: {x.shape}\")\n",
    "            print(f\"[PolicyNet.forward] fc1 weight shape: {self.fc1.weight.shape}, bias shape: {self.fc1.bias.shape}\")\n",
    "            self._debug_printed = True\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        probs = F.softmax(logits, dim=-1)  # [B, 2]\n",
    "        return probs\n",
    "    \n",
    "\n",
    "# ----------------- DP: exact J(θ) using policy p_s -----------------\n",
    "def build_system(model: PolicyNet, N: int, gamma: float,\n",
    "                 running_reward: float, terminal_reward: float):\n",
    "    \"\"\"\n",
    "    In tabular case, we could build A,b directly from the bellman's equations (eq 1.21 in lecture note).\n",
    "    Build linear system A V = b for states s=1..N-1.\n",
    "    Transition probabilities p_s come from the torch policy (Right prob).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"[build_system] N={N}, device={device}\")\n",
    "    with torch.no_grad():\n",
    "        s_idx = torch.arange(1, N, dtype=torch.float32, device=device)\n",
    "        print(f\"[build_system] s_idx shape: {s_idx.shape}\")\n",
    "        x = s_idx / float(N)                  # (N-1,)\n",
    "        print(f\"[build_system] x (before reshape) shape: {x.shape}\")\n",
    "        x = x.view(-1, 1)                     # Reshape to (N-1, 1) explicitly\n",
    "        print(f\"[build_system] x (after reshape) shape: {x.shape}\")\n",
    "        print(f\"[build_system] First layer input dim: {model.fc1.in_features}, output dim: {model.fc1.out_features}\")\n",
    "        pi = model(x)                         # (N-1, 2)\n",
    "        print(f\"[build_system] pi shape: {pi.shape}\")\n",
    "        p = pi[:, 1].cpu().numpy()            # P(right | s)\n",
    "        q = (1.0 - p)                         # P(left  | s)\n",
    "        print(f\"[build_system] p shape: {p.shape}, mean p(right): {p.mean():.3f}\")\n",
    "\n",
    "    A = np.zeros((N-1, N-1), dtype=np.float64)\n",
    "    b = np.zeros(N-1, dtype=np.float64)\n",
    "\n",
    "    if N - 1 == 1:\n",
    "        A[0, 0] = 1.0\n",
    "        b[0] = terminal_reward\n",
    "        return A, b\n",
    "\n",
    "    # s = 1 (index 0)\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = -gamma * p[0]\n",
    "    b[0] = q[0] * terminal_reward + p[0] * running_reward\n",
    "\n",
    "    # s = 2..N-2 (indices 1..N-3)\n",
    "    for s in range(2, N-1):\n",
    "        i = s - 1\n",
    "        A[i, i]   = 1.0\n",
    "        A[i, i-1] = -gamma * q[i]\n",
    "        A[i, i+1] = -gamma * p[i]\n",
    "        b[i]      = running_reward\n",
    "\n",
    "    # s = N-1 (index N-2)\n",
    "    i = N - 2\n",
    "    A[i, i]   = 1.0\n",
    "    A[i, i-1] = -gamma * q[i]\n",
    "    b[i]      = p[i] * terminal_reward + q[i] * running_reward\n",
    "    return A, b\n",
    "\n",
    "def get_V_and_J(model: PolicyNet, N: int, gamma: float,\n",
    "                running_reward: float, terminal_reward: float):\n",
    "    \"\"\"Solve A V = b; return V(s) for s=1..N-1 and uniform-start J.\"\"\"\n",
    "    A, b = build_system(model, N, gamma, running_reward, terminal_reward)\n",
    "    V = np.linalg.solve(A, b)\n",
    "    return V, float(V.mean())\n",
    "\n",
    "def get_Q(model: PolicyNet, N: int, gamma: float,\n",
    "          running_reward: float, terminal_reward: float):\n",
    "    \"\"\"\n",
    "    Q(s,a) via one-step lookahead using V from DP.\n",
    "    Returns Q for s=1..N-1 (shape (N-1, 2)).\n",
    "    \"\"\"\n",
    "    V, _ = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "    V_full = np.zeros(N + 1)\n",
    "    V_full[1:N] = V\n",
    "    s = np.arange(1, N, dtype=np.int64)\n",
    "    sL, sR = s - 1, s + 1\n",
    "    rL = np.where(sL == 0, terminal_reward, running_reward)\n",
    "    rR = np.where(sR == N, terminal_reward, running_reward)\n",
    "    Q = np.empty((N-1, 2), dtype=np.float64)\n",
    "    Q[:, 0] = rL + gamma * V_full[sL]\n",
    "    Q[:, 1] = rR + gamma * V_full[sR]\n",
    "    return Q\n",
    "\n",
    "# ----------------- Finite-difference gradient on θ (torch) -----------------\n",
    "def finite_difference_grad(model: PolicyNet, N: int, gamma: float,\n",
    "                           running_reward: float, terminal_reward: float,\n",
    "                           eps: float = 1e-4, relative: bool = False,\n",
    "                           scheme: str = 'central'):\n",
    "    \"\"\"\n",
    "    Finite-difference ∇θ J where θ is the concatenated torch parameter vector.\n",
    "    Supports central or forward difference. Optional relative step size.\n",
    "    \"\"\"\n",
    "    theta0 = parameters_to_vector(model.parameters()).detach().clone()\n",
    "    _, J0 = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "    grad = torch.zeros_like(theta0)\n",
    "\n",
    "    for i in range(theta0.numel()):\n",
    "        base = float(abs(theta0[i])) if relative else 1.0\n",
    "        h = eps * max(1.0, base)\n",
    "\n",
    "        if scheme.lower() == 'central':\n",
    "            th_p = theta0.clone(); th_p[i] += h\n",
    "            th_m = theta0.clone(); th_m[i] -= h\n",
    "            vector_to_parameters(th_p, model.parameters())\n",
    "            Jp = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            vector_to_parameters(th_m, model.parameters())\n",
    "            Jm = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            grad[i] = (Jp - Jm) / (2.0 * h)\n",
    "        elif scheme.lower() == 'forward':\n",
    "            th_p = theta0.clone(); th_p[i] += h\n",
    "            vector_to_parameters(th_p, model.parameters())\n",
    "            Jp = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            grad[i] = (Jp - J0) / h\n",
    "        else:\n",
    "            raise ValueError(\"scheme must be 'central' or 'forward'\")\n",
    "\n",
    "    # restore original params\n",
    "    vector_to_parameters(theta0, model.parameters())\n",
    "    return J0, grad.detach()\n",
    "\n",
    "# ----------------- MC gradient estimators (REINFORCE family) -----------------\n",
    "def mc_grad_estimators(model: PolicyNet, N: int, s0_batch: torch.Tensor, gamma: float,\n",
    "                       step_cost: float, terminal_reward: float,\n",
    "                       V: torch.Tensor = None, Q: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    We compute per-sample score vectors using autograd by calling backward()\n",
    "    on log π(a_t|s_t) to obtain ∇θ log π(a_t|s_t).\n",
    "\n",
    "    Returns (all numpy arrays):\n",
    "      g1..g6: (B, P) per-episode gradient samples; J: (B,)\n",
    "        g1: full-return REINFORCE\n",
    "        g2: return-to-go REINFORCE (via cumulative scores H)\n",
    "        g3: baseline with V(s_t)\n",
    "        g4: use Q(s_t, a_t)\n",
    "        g5: use Advantage A = Q − V\n",
    "        g6: use TD residual δ_t = r_t + γ V(s_{t+1}) − V(s_t)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"[mc_grad_estimators] Starting with N={N}, gamma={gamma}\")\n",
    "    print(f\"[mc_grad_estimators] device={device}\")\n",
    "    \n",
    "    names, base_params = zip(*list(model.named_parameters()))\n",
    "    # Detach so these are leaf tensors, then turn grad on (required by jacrev)\n",
    "    params = tuple(p.detach().requires_grad_(True) for p in base_params)\n",
    "    P = sum(p.numel() for p in params)\n",
    "    print(f\"[mc_grad_estimators] Number of parameters P={P}\")\n",
    "\n",
    "    B = int(s0_batch.numel())\n",
    "    print(f\"[mc_grad_estimators] Batch size B={B}\")\n",
    "    s    = s0_batch.to(device).clone()\n",
    "    done = (s == 0) | (s == N)\n",
    "    print(f\"[mc_grad_estimators] Initial states shape: {s.shape}, initial done count: {done.sum().item()}\")\n",
    "    \n",
    "    if V is not None:\n",
    "        print(f\"[mc_grad_estimators] V provided, shape: {V.shape}\")\n",
    "    if Q is not None:\n",
    "        print(f\"[mc_grad_estimators] Q provided, shape: {Q.shape}\")\n",
    "\n",
    "    H  = torch.zeros(B, P, device=device)  # cumulative score per-episode\n",
    "    g1 = torch.zeros(B, P, device=device)\n",
    "    g2 = torch.zeros(B, P, device=device)\n",
    "    g3 = torch.zeros(B, P, device=device)\n",
    "    g4 = torch.zeros(B, P, device=device)\n",
    "    g5 = torch.zeros(B, P, device=device)\n",
    "    g6 = torch.zeros(B, P, device=device)\n",
    "    J  = torch.zeros(B,   device=device)\n",
    "    gpw= torch.ones (B,   device=device)  # γ^t\n",
    "    \n",
    "    # Track trajectory for computing return-to-go properly\n",
    "    rewards_traj = []  # List of reward tensors per step\n",
    "    scores_traj = []   # List of score tensors per step  \n",
    "    states_traj = []   # List of state tensors per step\n",
    "    actions_traj = []  # List of action tensors per step\n",
    "    active_mask_traj = []  # Which episodes were active at each step\n",
    "\n",
    "    if V is not None:\n",
    "        V = V.to(device)     # shape N+1, suggest V[0]=V[N]=0\n",
    "    if Q is not None:\n",
    "        Q = Q.to(device)     # shape (N+1,2), with Q(0,.)=Q(N,.)=0 if you padded\n",
    "    \n",
    "    def logp_single(param_tensors, s_scalar: torch.Tensor, a_scalar: torch.Tensor):\n",
    "        # Build a param dict for functional_call\n",
    "        pmap = {n: t for n, t in zip(names, param_tensors)}\n",
    "        x = (s_scalar.float() / float(N)).view(1, 1)\n",
    "        probs = functional_call(model, pmap, (x,))    # (1,2)\n",
    "        # Differentiable action selection via gather (avoid data-dependent indexing pitfalls)\n",
    "        logp = probs.log().gather(1, a_scalar.long().view(1, 1)).squeeze()  # scalar\n",
    "        return logp\n",
    "\n",
    "    # Note: you may found this function useful, this calculate ∇θ log π(a_i|s_i) in a batch manner\n",
    "    def score_matrix_batch(active_s: torch.Tensor, active_a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns (B_act, P) where each row is ∇θ log π(a_i|s_i),\n",
    "        computed efficiently via vmap(jacrev) over (s,a).\n",
    "        \"\"\"\n",
    "        # jac is a pytree of tensors matching param shapes, each with leading dim B_act\n",
    "        jac = vmap(jacrev(logp_single), in_dims=(None, 0, 0))(params, active_s, active_a)\n",
    "        # Flatten each param’s jacobian and concatenate along feature dim\n",
    "        parts = [g.reshape(g.shape[0], -1) for g in jac]\n",
    "        return torch.cat(parts, dim=1)  # (B_act, P)\n",
    "\n",
    "\n",
    "    step_count = 0\n",
    "    while not torch.all(done):\n",
    "        step_count += 1\n",
    "        idx = (~done).nonzero(as_tuple=False).squeeze(1)\n",
    "        if idx.numel() == 0:\n",
    "            break\n",
    "        \n",
    "        if step_count % 1000 == 0:\n",
    "            print(f\"[mc_grad_estimators] Step {step_count}, active episodes: {idx.numel()}/{B}, done: {done.sum().item()}\")\n",
    "\n",
    "        # Sample actions for all active states in one forward pass\n",
    "        x = (s[idx].float() / float(N)).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            probs = model(x)                            # (B_act, 2)\n",
    "            a = torch.multinomial(probs, 1).squeeze(1) # (B_act,)\n",
    "\n",
    "        # ∇θ log π(a|s): (B_act, P) using backward()\n",
    "        score_mat = score_matrix_batch(s[idx], a)\n",
    "\n",
    "        # Next state, termination, and rewards\n",
    "        s_next   = s[idx] + torch.where(a == 1, 1, -1)\n",
    "        term_any = (s_next == 0) | (s_next == N)\n",
    "        r_t = torch.where(\n",
    "            term_any,\n",
    "            torch.tensor(terminal_reward, device=device),\n",
    "            torch.tensor(step_cost, device=device)\n",
    "        ).float()\n",
    "        \n",
    "        #########################################\n",
    "        #TODO 2.1: Implement the six policy gradient estimators\n",
    "        #########################################\n",
    "        # Store trajectory data for computing return-to-go\n",
    "        # CRITICAL: Detach when storing to avoid keeping computation graphs in memory!\n",
    "        # This prevents memory bloat from storing gradient history\n",
    "        rewards_traj.append(r_t.detach().clone())\n",
    "        scores_traj.append(score_mat.detach().clone())  # score_mat has gradients - must detach!\n",
    "        states_traj.append(s[idx].detach().clone())\n",
    "        actions_traj.append(a.detach().clone())\n",
    "        active_mask_traj.append(idx.detach().clone())\n",
    "        \n",
    "        # Detach score_mat immediately to avoid building massive computation graphs\n",
    "        # We need score_mat for this step's computations, then we can detach\n",
    "        score_mat_detached = score_mat.detach()\n",
    "        \n",
    "        # Accumulate cumulative score: H[idx] += score_mat (keep gradients for final g1)\n",
    "        # Note: This builds a computation graph, but it's necessary for REINFORCE\n",
    "        # We'll handle this by computing g1 from stored scores if memory is an issue\n",
    "        H[idx] += score_mat\n",
    "        \n",
    "        # Update returns: J[idx] += γ^t * r_t (total return) - detach r_t to avoid graph growth\n",
    "        J[idx] += gpw[idx] * r_t.detach()\n",
    "        \n",
    "        # Current state values (if V available)\n",
    "        v_curr = V[s[idx]] if V is not None else None\n",
    "        v_next = V[s_next] if V is not None else None\n",
    "        \n",
    "        # Current Q-values (if Q available)\n",
    "        q_sa = None\n",
    "        if Q is not None:\n",
    "            # Q[s, a] where s is current state, a is action taken\n",
    "            q_sa = torch.where(a == 1, Q[s[idx], 1], Q[s[idx], 0])  # [B_act]\n",
    "        \n",
    "        # Update g4: Use Q(s_t, a_t) * γ^t * ∇log π\n",
    "        # Use detached score_mat to avoid building computation graphs over thousands of steps\n",
    "        if Q is not None and q_sa is not None:\n",
    "            g4[idx] += (q_sa.unsqueeze(1) * gpw[idx].unsqueeze(1) * score_mat_detached).detach()\n",
    "        \n",
    "        # Update g5: Advantage A = Q(s_t, a_t) - V(s_t) * γ^t * ∇log π\n",
    "        if Q is not None and V is not None and q_sa is not None and v_curr is not None:\n",
    "            advantage = q_sa - v_curr  # [B_act]\n",
    "            g5[idx] += (advantage.unsqueeze(1) * gpw[idx].unsqueeze(1) * score_mat_detached).detach()\n",
    "        \n",
    "        # Update g6: TD residual δ_t = r_t + γV(s_{t+1}) - V(s_t) * γ^t * ∇log π\n",
    "        if V is not None and v_curr is not None and v_next is not None:\n",
    "            td_residual = r_t.detach() + gamma * v_next - v_curr  # [B_act]\n",
    "            g6[idx] += (td_residual.unsqueeze(1) * gpw[idx].unsqueeze(1) * score_mat_detached).detach()\n",
    "        \n",
    "        # Update discount factor for next step\n",
    "        gpw[idx] *= gamma\n",
    "        \n",
    "        # Update states\n",
    "        s[idx] = s_next\n",
    "        done[idx] = term_any\n",
    "    \n",
    "    print(f\"[mc_grad_estimators] Loop finished after {step_count} steps\")\n",
    "    print(f\"[mc_grad_estimators] Total trajectory steps recorded: {len(rewards_traj)}\")\n",
    "    print(f\"[mc_grad_estimators] Processing {B} episodes for return-to-go computation...\")\n",
    "    \n",
    "    # After all episodes finish, compute g1, g2, g3 using full trajectories\n",
    "    # For each episode, compute return-to-go backward and accumulate gradients\n",
    "    episodes_processed = 0\n",
    "    for ep_idx in range(B):\n",
    "        # Find when this episode was active and its position in active batch\n",
    "        episode_steps = []\n",
    "        for step_idx in range(len(rewards_traj)):\n",
    "            active_indices = active_mask_traj[step_idx]\n",
    "            mask = (active_indices == ep_idx)\n",
    "            if mask.any():\n",
    "                pos_in_active = mask.nonzero(as_tuple=False)[0].item()\n",
    "                episode_steps.append((step_idx, pos_in_active))\n",
    "        \n",
    "        if len(episode_steps) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Compute return-to-go backward: G_t = r_t + γ * G_{t+1}\n",
    "        returns_to_go = []\n",
    "        G = 0.0\n",
    "        for i in range(len(episode_steps) - 1, -1, -1):\n",
    "            step_idx, pos_idx = episode_steps[i]\n",
    "            r = rewards_traj[step_idx][pos_idx].item()\n",
    "            G = r + gamma * G\n",
    "            returns_to_go.insert(0, G)\n",
    "        \n",
    "        # Accumulate gradients for this episode\n",
    "        discount = 1.0\n",
    "        for i, (step_idx, pos_idx) in enumerate(episode_steps):\n",
    "            score = scores_traj[step_idx][pos_idx:pos_idx+1]  # [1, P]\n",
    "            G_t = returns_to_go[i]\n",
    "            \n",
    "            # g2: Return-to-go REINFORCE - G_t * γ^t * ∇log π\n",
    "            g2[ep_idx] += discount * G_t * score.squeeze(0)\n",
    "            \n",
    "            # g3: Baseline with V - (G_t - V(s_t)) * γ^t * ∇log π\n",
    "            if V is not None:\n",
    "                s_val = int(states_traj[step_idx][pos_idx].item())\n",
    "                v_s = V[s_val].item()\n",
    "                g3[ep_idx] += discount * (G_t - v_s) * score.squeeze(0)\n",
    "            \n",
    "            discount *= gamma\n",
    "        \n",
    "        episodes_processed += 1\n",
    "        if episodes_processed % 5000 == 0:\n",
    "            print(f\"[mc_grad_estimators] Processed {episodes_processed}/{B} episodes...\")\n",
    "    \n",
    "    print(f\"[mc_grad_estimators] Finished processing all {episodes_processed} episodes\")\n",
    "    \n",
    "    # Set g1: Full return REINFORCE = G(τ) * Σ_t ∇log π\n",
    "    # Compute in chunks to avoid memory issues with large tensors\n",
    "    print(f\"[mc_grad_estimators] Computing g1 from J and H...\")\n",
    "    print(f\"[mc_grad_estimators] J shape: {J.shape}, H shape: {H.shape}\")\n",
    "    \n",
    "    # Compute g1 in a memory-efficient way\n",
    "    J_expanded = J.unsqueeze(1)  # (B, 1)\n",
    "    g1 = (J_expanded * H).detach()\n",
    "    del J_expanded  # Free immediately\n",
    "    gc.collect()\n",
    "    \n",
    "    # Skip explicit detaching - they're already detached from computation\n",
    "    # The conversion will call detach() anyway, so we don't need to do it here\n",
    "    import sys\n",
    "    print(f\"[mc_grad_estimators] Tensors are already detached from computation\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Clear H after g1 is computed\n",
    "    print(f\"[mc_grad_estimators] Clearing H tensor...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        del H\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] H cleared\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Warning clearing H: {e}\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Print shapes carefully to avoid issues\n",
    "    print(f\"[mc_grad_estimators] Printing final statistics...\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Final shapes: g1={g1.shape}, g2={g2.shape}, J={J.shape}\", flush=True)\n",
    "        print(f\"[mc_grad_estimators] g1.requires_grad={g1.requires_grad}, g2.requires_grad={g2.requires_grad}\", flush=True)\n",
    "        j_mean = J.mean().item()\n",
    "        j_std = J.std().item()\n",
    "        print(f\"[mc_grad_estimators] J mean: {j_mean:.3f}, std: {j_std:.3f}\", flush=True)\n",
    "        sys.stdout.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Warning during shape printing: {e}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Clear trajectory data to free memory before conversion\n",
    "    print(f\"[mc_grad_estimators] Clearing trajectory data to free memory...\")\n",
    "    print(f\"[mc_grad_estimators] Trajectory lists length: rewards={len(rewards_traj)}, scores={len(scores_traj)}\")\n",
    "    try:\n",
    "        # Clear lists one by one\n",
    "        del rewards_traj\n",
    "        gc.collect()\n",
    "        del scores_traj\n",
    "        gc.collect()\n",
    "        del states_traj\n",
    "        gc.collect()\n",
    "        del actions_traj\n",
    "        gc.collect()\n",
    "        del active_mask_traj\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] Trajectory data cleared successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Warning: Error clearing trajectory data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Force multiple garbage collection passes to ensure memory is freed\n",
    "    for i in range(3):\n",
    "        collected = gc.collect()\n",
    "        if collected == 0:\n",
    "            break\n",
    "    \n",
    "    # Check memory before conversion\n",
    "    try:\n",
    "        import psutil\n",
    "        import os\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"[mc_grad_estimators] Memory usage before conversion: {mem_mb:.2f} MB\")\n",
    "    except ImportError:\n",
    "        print(f\"[mc_grad_estimators] psutil not available, skipping memory check\")\n",
    "    \n",
    "    # Convert tensors to numpy one at a time to avoid memory issues\n",
    "    # Since we're on CPU, convert directly but in stages\n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g1 to numpy...\")\n",
    "        print(f\"[mc_grad_estimators] g1 requires_grad: {g1.requires_grad}, device: {g1.device}\")\n",
    "        g1_np = g1.detach().numpy()  # Already on CPU, just detach and convert\n",
    "        print(f\"[mc_grad_estimators] g1_np shape: {g1_np.shape}, dtype: {g1_np.dtype}, size: {g1_np.nbytes / 1024 / 1024:.2f} MB\")\n",
    "        del g1  # Delete tensor immediately\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] g1 conversion successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g2 to numpy...\")\n",
    "        g2_np = g2.detach().numpy()\n",
    "        print(f\"[mc_grad_estimators] g2_np shape: {g2_np.shape}, dtype: {g2_np.dtype}, size: {g2_np.nbytes / 1024 / 1024:.2f} MB\")\n",
    "        del g2\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g3 to numpy...\")\n",
    "        if V is not None:\n",
    "            g3_np = g3.detach().numpy()\n",
    "            del g3\n",
    "        else:\n",
    "            g3_np = np.zeros_like(g2_np)\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] g3_np shape: {g3_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g4 to numpy...\")\n",
    "        if Q is not None:\n",
    "            g4_np = g4.detach().numpy()\n",
    "            del g4\n",
    "        else:\n",
    "            g4_np = np.zeros_like(g2_np)\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] g4_np shape: {g4_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g5 to numpy...\")\n",
    "        if V is not None and Q is not None:\n",
    "            g5_np = g5.detach().numpy()\n",
    "            del g5\n",
    "        else:\n",
    "            g5_np = np.zeros_like(g2_np)\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] g5_np shape: {g5_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g5: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting g6 to numpy...\")\n",
    "        if V is not None:\n",
    "            g6_np = g6.detach().numpy()\n",
    "            del g6\n",
    "        else:\n",
    "            g6_np = np.zeros_like(g2_np)\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] g6_np shape: {g6_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting g6: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        print(f\"[mc_grad_estimators] Converting J to numpy...\")\n",
    "        J_np = J.detach().numpy()\n",
    "        del J  # H was already deleted earlier\n",
    "        gc.collect()\n",
    "        print(f\"[mc_grad_estimators] J_np shape: {J_np.shape}, size: {J_np.nbytes / 1024:.2f} KB\")\n",
    "    except Exception as e:\n",
    "        print(f\"[mc_grad_estimators] Error converting J: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    print(f\"[mc_grad_estimators] All conversions complete!\")\n",
    "    \n",
    "    return (g1_np, g2_np, g3_np, g4_np, g5_np, g6_np, J_np)\n",
    "\n",
    "# ----------------- main -----------------\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Problem 2.1: Policy Gradient Estimators\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"[main] Using device: {device}\")\n",
    "\n",
    "# Environment\n",
    "N = 10\n",
    "gamma = 0.9\n",
    "running_reward = -1.0\n",
    "terminal_reward = 100.0\n",
    "print(f\"[main] Environment: N={N}, gamma={gamma}, running_reward={running_reward}, terminal_reward={terminal_reward}\")\n",
    "\n",
    "# Policy\n",
    "print(f\"[main] Creating PolicyNet with hidden=2...\")\n",
    "model = PolicyNet(hidden=2).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"[main] Model created with {total_params} total parameters\")\n",
    "print(f\"[main] Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Exact J and FD gradient\n",
    "print(f\"[main] Computing exact J and finite-difference gradient...\")\n",
    "J_true, grad_fd = finite_difference_grad(\n",
    "    model, N, gamma, running_reward, terminal_reward,\n",
    "    eps=1e-4, relative=True, scheme='central'\n",
    ")\n",
    "print(f\"[main] J_true = {J_true:.6f}, grad_fd shape = {grad_fd.shape}, grad_fd norm = {grad_fd.norm().item():.6f}\")\n",
    "\n",
    "# Baselines from DP value V(s) and Q(s,a)\n",
    "print(f\"[main] Computing V and Q from dynamic programming...\")\n",
    "V_np, _ = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "V = torch.tensor(np.concatenate(([0.0], V_np, [0.0])), dtype=torch.float32, device=device)  # V(0)=V(N)=0\n",
    "print(f\"[main] V shape: {V.shape}, V range: [{V.min().item():.3f}, {V.max().item():.3f}]\")\n",
    "\n",
    "Q_np = get_Q(model, N, gamma, running_reward, terminal_reward).astype(np.float32)\n",
    "Q_t  = torch.tensor(Q_np, dtype=torch.float32, device=device)\n",
    "zero_row = torch.zeros(1, 2, dtype=torch.float32, device=device)\n",
    "Q = torch.cat([zero_row, Q_t, zero_row], dim=0)  # Q(0,.)=Q(N,.)=0\n",
    "print(f\"[main] Q shape: {Q.shape}, Q range: [{Q.min().item():.3f}, {Q.max().item():.3f}]\")\n",
    "\n",
    "# Monte Carlo (batched episodes)\n",
    "episodes = 20000  # adjust as needed\n",
    "print(f\"[main] Starting Monte Carlo gradient estimation with {episodes} episodes...\")\n",
    "s0_batch = torch.randint(1, N, (episodes,), dtype=torch.int64, device=device)\n",
    "print(f\"[main] Initial state batch shape: {s0_batch.shape}, range: [{s0_batch.min().item()}, {s0_batch.max().item()}]\")\n",
    "\n",
    "g1, g2, g3, g4, g5, g6, J = mc_grad_estimators(\n",
    "    model, N, s0_batch, gamma,\n",
    "    step_cost=running_reward,\n",
    "    terminal_reward=terminal_reward,\n",
    "    V=V, Q=Q\n",
    ")\n",
    "\n",
    "print(f\"[main] Monte Carlo estimation completed!\")\n",
    "print(f\"[main] Returned shapes: g1={g1.shape}, g2={g2.shape}, g3={g3.shape}\")\n",
    "print(f\"[main] Returned shapes: g4={g4.shape}, g5={g5.shape}, g6={g6.shape}, J={J.shape}\")\n",
    "\n",
    "#########################################\n",
    "#TODO 2.1: Plot your result here\n",
    "# 1. Print out the standard deviation of each gradient estimator\n",
    "# 2. Plot the running error of the estimated J vs the true J, you may found np.cumsum(:, axis=0) / np.arange(1, len(J) + 1) useful\n",
    "# 3. Plot the running error of each gradient estimator vs the FD gradient\n",
    "#########################################\n",
    "\n",
    "# Convert FD gradient to numpy for comparison\n",
    "grad_fd_np = grad_fd.cpu().numpy()\n",
    "\n",
    "# 1. Print standard deviations\n",
    "grad_names = ['g1 (REINFORCE)', 'g2 (Return-to-go)', 'g3 (Baseline V)', \n",
    "              'g4 (Q)', 'g5 (Advantage)', 'g6 (TD residual)']\n",
    "grads = [g1, g2, g3, g4, g5, g6]\n",
    "\n",
    "print(\"Standard deviations of gradient estimators:\")\n",
    "for name, g in zip(grad_names, grads):\n",
    "    if g.size > 0:\n",
    "        std_per_param = np.std(g, axis=0)  # std across episodes for each parameter\n",
    "        mean_std = np.mean(std_per_param)\n",
    "        print(f\"{name}: mean_std = {mean_std:.6f}\")\n",
    "\n",
    "# 2. Compute running mean of J and plot error vs true J\n",
    "J_running_mean = np.cumsum(J) / np.arange(1, len(J) + 1)\n",
    "J_error = np.abs(J_running_mean - J_true)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(J_error)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('|J_bar_k - J_true|')\n",
    "plt.title('Running Error of Estimated J vs True J')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3. Compute running mean gradients and plot error vs FD gradient\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, g) in enumerate(zip(grad_names, grads)):\n",
    "    if g.size > 0:\n",
    "        # Running mean gradient: average over first k episodes\n",
    "        g_running_mean = np.cumsum(g, axis=0) / np.arange(1, len(g) + 1)[:, None]\n",
    "        \n",
    "        # Error: L2 norm of difference from FD gradient for each episode\n",
    "        grad_error = np.linalg.norm(g_running_mean - grad_fd_np, axis=1)\n",
    "        \n",
    "        axes[idx].plot(grad_error)\n",
    "        axes[idx].set_xlabel('Episode')\n",
    "        axes[idx].set_ylabel('||g_bar_k - grad_FD||_2')\n",
    "        axes[idx].set_title(name)\n",
    "        axes[idx].set_yscale('log')\n",
    "        axes[idx].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 REINFORCE algorithm\n",
    "\n",
    "**Algorithm Recap — REINFORCE (Monte-Carlo Policy Gradient)**\n",
    "\n",
    "1. **Policy network**  \n",
    "   Stochastic policy $\\pi_\\theta(a\\mid s)$ \n",
    "\n",
    "2. **Trajectory sampling**  \n",
    "   Roll out episodes with $\\pi_\\theta$: $(s_1,a_1,r_1,\\dots,s_T,a_T,r_T)$.\n",
    "\n",
    "3. **Returns / advantages**\n",
    "   - Monte-Carlo return:\n",
    "     $$\n",
    "     G_t=\\sum_{t'=t}^{T}\\gamma^{\\,t'-t} r_{t'}.\n",
    "     $$\n",
    "   - Advantage: $A_t = G_t - b(s_t)$.\n",
    "\n",
    "4. **Policy-gradient update**\n",
    "   - Estimator:\n",
    "     $$\n",
    "     \\hat g(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T_i}\n",
    "       \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\, \\gamma^t A_t^{(i)}.\n",
    "     $$\n",
    "   - Gradient ascent: $\\theta \\leftarrow \\theta + \\alpha\\,\\hat g(\\theta)$.\n",
    "\n",
    "5. **Learned value baseline (optional)**\n",
    "   - Regress $V_\\psi(s)$ to returns:\n",
    "     $$\n",
    "     \\min_\\psi \\frac{1}{N}\\sum_{i,t}\\big(V_\\psi(s_t^{(i)})-G_t^{(i)}\\big)^2,\n",
    "     \\qquad A_t\\!=\\!G_t\\!-\\!V_\\psi(s_t).\n",
    "     $$\n",
    "\n",
    "6. **Mini-batch training**\n",
    "   - Collect $N$ episodes (or $M$ steps), compute $G_t/A_t$; optimize\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{PG}}(\\theta)=\n",
    "     -\\frac{1}{N}\\sum_{i,t}\\log\\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\,A_t^{(i)}.\n",
    "     $$\n",
    "\n",
    "**TODO:**\n",
    "- implement policy net and value net\n",
    "- implement the main algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Acrobot-v1\"\n",
    "SEED = 0\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.995\n",
    "\n",
    "LR_POLICY = 3e-4\n",
    "LR_VALUE  = 1e-3\n",
    "MAX_EPOCHS = 400\n",
    "BATCH_SIZE = 16\n",
    "MAX_EP_LEN = 1000\n",
    "VALUE_UPDATES = 10\n",
    "\n",
    "GRAD_CLIP = 10.0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]   # 6 for Acrobot\n",
    "act_dim = env.action_space.n               # 3 for Acrobot\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# ------------------- tensor wrapper (given as a reference) -------------------\n",
    "class TWrapper(gym.Wrapper):\n",
    "    def __init__(self, env): super().__init__(env)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        return torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0), info\n",
    "    def step(self, action):\n",
    "        a = int(action.item()) if isinstance(action, torch.Tensor) else int(action)\n",
    "        obs, r, term, trunc, info = self.env.step(a)\n",
    "        done = bool(term or trunc)\n",
    "        obs_t = torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0)\n",
    "        r_t   = torch.tensor([[r]], dtype=torch.float32)\n",
    "        d_t   = torch.tensor([[done]], dtype=torch.bool)\n",
    "        return obs_t, r_t, d_t, info\n",
    "    \n",
    "env = TWrapper(env)\n",
    "\n",
    "# ------------------- discrete policy net (given as a reference) -------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement policy network\n",
    "    #########################################\n",
    "    \n",
    "# ------------------- value baseline (given as a reference) -------------------\n",
    "class ValueNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement value network\n",
    "    #########################################\n",
    "    \n",
    "    \n",
    "policy = PolicyNet(obs_dim=obs_dim, hidden=HIDDEN, act_dim=act_dim).to(DEVICE)\n",
    "vnet   = ValueNet(obs_dim=obs_dim, hidden=HIDDEN).to(DEVICE)\n",
    "\n",
    "# ------------------- utils -------------------\n",
    "def mc_returns_single_traj(R: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    \"\"\"R: [T] -> G: [T], reverse within a single trajectory.\"\"\"\n",
    "    G = torch.zeros_like(R)\n",
    "    running = 0.0\n",
    "    for t in range(R.numel() - 1, -1, -1):\n",
    "        running = R[t] + gamma * running\n",
    "        G[t] = running\n",
    "    return G\n",
    "\n",
    "# ------------------- training -------------------\n",
    "def train():\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement vanilla REINFORCE algorithm\n",
    "    #########################################\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "    return policy, vnet, returns_history\n",
    "\n",
    "policy, vnet, returns_history = train()\n",
    "\n",
    "def eval(policy, episodes=10, greedy=True, device=DEVICE, max_len=MAX_EP_LEN):\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "    policy.eval()\n",
    "    succ, max_hs = [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(episodes):\n",
    "            o, _ = env.reset()\n",
    "            ok, m = False, -1e9\n",
    "            for _ in range(max_len):\n",
    "                s = torch.as_tensor(o, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                logits = policy(s)\n",
    "                a = int(logits.argmax(-1)) if greedy else int(Categorical(logits=logits).sample())\n",
    "                o, r, term, trunc, _ = env.step(a)\n",
    "                c1, s1, c2, s2 = o[:4]; m = max(m, float(-c1 - (c1*c2 - s1*s2)))  # tip height\n",
    "                if term or trunc: ok = bool(term); break\n",
    "            succ.append(ok); max_hs.append(m)\n",
    "    print(f\"success={np.mean(succ):.1%}, mean_max_tip={np.mean(max_hs):.3f}\")\n",
    "    \n",
    "eval(policy, episodes=100, greedy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE with learned value function often have high variance (recall what we find in PSET2 2.1). Actor-critic method replace the advantage $A_t = G_t - b(s_t)$ with temporal-difference error $r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "**Algorithm recap**\n",
    "\n",
    "1. **Networks**\n",
    "   - **Actor**: stochastic policy $ \\pi_\\theta(a\\mid s) $.\n",
    "   - **Critic**: value $ V_\\psi(s) $\n",
    "  \n",
    "2. **Data collection**\n",
    "   \n",
    "   Roll out for $n$ steps (or full episodes) with $\\pi_\\theta$; store $(s_t,a_t,r_t,s_{t+1},\\text{done}_t)$.\n",
    "\n",
    "3. **TD advantage (one-step)**\n",
    "   $$\n",
    "   y_t = r_t + \\gamma \\,V_{\\psi}(s_{t+1}),\\qquad\n",
    "   \\delta_t = y_t - V_\\psi(s_t).\n",
    "   $$\n",
    "   Use $\\delta_t$ as **advantage** (variance lower than Monte-Carlo $G_t$).\n",
    "\n",
    "4. **Losses**\n",
    "   - **Actor**  \n",
    "     $$\n",
    "     \\mathcal L_{\\pi}(\\theta)\n",
    "     = -\\,\\mathbb E\\big[\\,\\log \\pi_\\theta(a_t\\!\\mid s_t)\\,\\delta_t \\big]\n",
    "     $$\n",
    "   - **Critic**  \n",
    "     $$\n",
    "     \\mathcal L_V(\\psi) = \\tfrac12\\,\\mathbb E\\big[(V_\\psi(s_t)-y_t)^2\\big].\n",
    "     $$\n",
    "\n",
    "Several other features you may consider:\n",
    "- Multi-step update for value function \n",
    "- Normalize the advantage over batch\n",
    "\n",
    "**TODO:**\n",
    "- implement policy net and value net\n",
    "- implement the main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-policy Actor–Critic for Acrobot-v1\n",
    "# - Discrete actions, update every K steps (no need to finish episodes)\n",
    "\n",
    "import math, random\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Acrobot-v1\"\n",
    "SEED = 0\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.995\n",
    "\n",
    "LR_POLICY = 3e-4\n",
    "LR_VALUE  = 1e-3\n",
    "MAX_EPOCHS = 500\n",
    "STEPS_PER_UPDATE = 64           # ← collect this many steps, then update (true on-policy)\n",
    "\n",
    "CRITIC_UPDATES = 1              # critic updates per actor step\n",
    "GRAD_CLIP = 10.0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]   # 6 for Acrobot\n",
    "act_dim = env.action_space.n               # 3 for Acrobot\n",
    "\n",
    "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "# ------------------- networks -------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement policy network\n",
    "    #########################################\n",
    "    \n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement value network\n",
    "    #########################################\n",
    "    \n",
    "\n",
    "policy = PolicyNet(obs_dim, HIDDEN, act_dim).to(DEVICE)\n",
    "value  = ValueNet(obs_dim, HIDDEN).to(DEVICE)\n",
    "opt_pi = torch.optim.Adam(policy.parameters(), lr=LR_POLICY)\n",
    "opt_v  = torch.optim.Adam(value.parameters(),  lr=LR_VALUE)\n",
    "\n",
    "# ------------------- helper -------------------\n",
    "@torch.no_grad()\n",
    "def to_t(s): return torch.as_tensor(s, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "# ------------------- training (A2C / 1-step actor-critic) -------------------\n",
    "def train():\n",
    "    returns_history: List[float] = []\n",
    "    ep_ret, ep_len = 0.0, 0\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement the main algorithm\n",
    "    #########################################\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "    return policy, value, returns_history\n",
    "\n",
    "policy, value, returns = train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: PPO for pendulum\n",
    "\n",
    "Vanilla actor-critic often face the problem of distribution shift. Advanced actor-critic deal with this problem by adding trust region constraints. PPO is the most famous and widely-used one in robotics. In this problem you will implement PPO on gym's `pendulum-v1` environment.\n",
    "\n",
    "### Environment & action space\n",
    "\n",
    "- **Env**: `Pendulum-v1` (pendulum swing-up) [Link](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "- **Observation**: 3-D vector $[\\cos\\theta,\\ \\sin\\theta,\\ \\dot\\theta]$.\n",
    "- **Actions**: Continuous torque, shape $(1,)$, range $[-2, 2]$ (env clips to bounds).\n",
    "- **Reward**: \n",
    "  $$ r = -\\big(\\theta^2 + 0.1\\,\\dot\\theta^{\\,2} + 0.001\\,u^{2}\\big) $$\n",
    "  where $\\theta\\in(-\\pi,\\pi]$ is angle to upright ($0$ is upright), $\\dot\\theta$ is angular velocity, and $u$ is applied torque. Maximized when the pendulum is upright and still with minimal torque.\n",
    "\n",
    "\n",
    "### Algorithm Recap\n",
    "\n",
    "**Policy & Value.**\n",
    "- Policy: Gaussian $\\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))$.\n",
    "- Critic: scalar value $V_\\phi(s)$.\n",
    "\n",
    "**Data collection (on-policy).**\n",
    "- Roll out episodes using the current policy, storing $(s_t,a_t,r_t,s_{t+1},d_t)$.\n",
    "\n",
    "**Targets and Advantage.**\n",
    "- One-step TD target: $\\hat{V}_t = r_t + \\gamma V_\\phi(s_{t+1})$.\n",
    "- TD residual: $\\delta_t = \\hat{V}_t - V_\\phi(s_t)$.\n",
    "- GAE($\\lambda$) advantage:\n",
    "  $$\n",
    "  \\hat{A}_t = \\sum_{k=0}^{\\infty} (\\gamma\\lambda)^k \\, \\delta_{t+k}.\n",
    "  $$\n",
    "  (Computed by a backward recursion.)\n",
    "\n",
    "**PPO-Clip objective.**\n",
    "- Log-ratio $r_t(\\theta) = \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t\\mid s_t)}$.\n",
    "- Clipped surrogate:\n",
    "  $$\n",
    "  \\mathcal{L}^{\\text{CLIP}}(\\theta)\n",
    "  = \\mathbb{E}\\Big[\\min\\big(r_t(\\theta)\\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\big)\\Big].\n",
    "  $$\n",
    "- Value loss: $\\mathcal{L}_V = \\|V_\\phi(s_t) - \\hat{V}_t\\|_2^2$.\n",
    "- Total loss (per minibatch): $-\\mathcal{L}^{\\text{CLIP}} + c_v \\mathcal{L}_V$ (entropy term optional).\n",
    "\n",
    "**Update.**\n",
    "- Cache old log-probs once per batch.\n",
    "- For several **epochs**, shuffle the batch and optimize the total loss on minibatches (Adam).\n",
    "\n",
    "**TODO:** Implement a complete PPO agent from scratch, using the provided scaffold and suggested hyperparameters as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Pendulum-v1\"\n",
    "SEED = 0\n",
    "\n",
    "LR_POLICY = 1e-4\n",
    "LR_VALUE  = 5e-3\n",
    "NUM_EPSIODE = 3000       # (kept your variable name)\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.9\n",
    "VF_COEF = 0.9            # value loss weight in the total loss\n",
    "UPDATE_EPOCHS = 10       # PPO epochs per update\n",
    "CLIP_EPS = 0.2           # PPO clipping epsilon\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]   # 3 for Pendulum\n",
    "action_dim = env.action_space.shape[0]        # 1 for Pendulum\n",
    "\n",
    "#########################################\n",
    "#TODO 4: Implement PPO\n",
    "#########################################\n",
    "\n",
    "# ------------------- utils -------------------\n",
    "def compute_advantage(gamma: float, lmbda: float, td_delta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pure torch GAE-style backward recursion to avoid NumPy conversions.\n",
    "    td_delta: [T,1] or [T]; returns [T,1].\n",
    "    \"\"\"\n",
    "    td = td_delta.view(-1)                # [T]\n",
    "    adv = torch.zeros_like(td)\n",
    "    gae = torch.zeros(1, dtype=td.dtype, device=td.device)\n",
    "    for t in range(td.shape[0] - 1, -1, -1):\n",
    "        gae = gamma * lmbda * gae + td[t]\n",
    "        adv[t] = gae\n",
    "    return adv.view(-1, 1)\n",
    "\n",
    "# ------------------- PPO (continuous) -------------------\n",
    "class PPOContinuous:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, vf_coef, gamma, device):\n",
    "        self.actor  = PolicyNetContinuous(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer  = torch.optim.Adam(self.actor.parameters(),  lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma  = gamma\n",
    "        self.lmbda  = lmbda\n",
    "        self.epochs = epochs\n",
    "        self.eps    = eps\n",
    "        self.vf_coef = vf_coef\n",
    "        self.device = device\n",
    "\n",
    "# ------------------- training loop (Gymnasium API) -------------------\n",
    "def train_on_policy_agent(env, agent, num_episodes):\n",
    "    \n",
    "\n",
    "# ------------------- run -------------------\n",
    "agent = PPOContinuous(state_dim, HIDDEN, action_dim, LR_POLICY, LR_VALUE,\n",
    "                      LAMBDA, UPDATE_EPOCHS, CLIP_EPS, VF_COEF, GAMMA, DEVICE)\n",
    "return_list = train_on_policy_agent(env, agent, NUM_EPSIODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Mujoco Half-cheetch envornment with stable baseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will use gym's [Mujoco](https://gymnasium.farama.org/environments/mujoco/) environment and [stable baseline3](https://stable-baselines3.readthedocs.io/en/master/) to train a PPO network on Half-cheetah environment.\n",
    "\n",
    "### Half-cheetah\n",
    "This environment is based on the work of P. Wawrzyński in “A Cat-Like Robot Real-Time Learning to Run”. The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torque to the joints to make the cheetah run forward (right) as fast as possible, with a positive reward based on the distance moved forward and a negative reward for moving backward. \n",
    "\n",
    "Download it using `pip install \"gymnasium[mujoco]\"`\n",
    "\n",
    "### Stable baseline 3\n",
    "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. You can directly load `PPO` module from the repo and define the hyper-parameter yourselves.\n",
    "\n",
    "Download it using `pip install 'stable-baselines3[extra]'`\n",
    "\n",
    "**TODO:** Tune the parameter yourself, what's your feeling about different parameters?\n",
    "\n",
    "Note: the output is printed in the `logs/progress.csv` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "save = \"ckpt/half_cheetah_ppo\"\n",
    "\n",
    "env = Monitor(gym.make(\"HalfCheetah-v4\"))\n",
    "\n",
    "#########################################\n",
    "#TODO 5: Change the parameter yourself to finish training\n",
    "#########################################\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=dict(\n",
    "        log_std_init=-2,\n",
    "        ortho_init=False,\n",
    "        activation_fn=nn.ReLU,\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256]),\n",
    "    ),\n",
    "    # PPO clipping parameter\n",
    "    clip_range=0.2,\n",
    "    # entropy coefficient\n",
    "    ent_coef=0.0004,\n",
    "    # GAE lambda parameter\n",
    "    gae_lambda=0.92,\n",
    "    gamma=0.98,\n",
    "    learning_rate=2.5e-5,\n",
    "    max_grad_norm=0.8,\n",
    "    n_steps=int(512*4),\n",
    "    # number of epochs when optimizing one batch\n",
    "    n_epochs=20,\n",
    "    device=\"cpu\",\n",
    "    # value function coefficient in the loss\n",
    "    vf_coef=0.5,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "new_logger = configure(\"logs\", [\"csv\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "n_envs = model.n_envs     \n",
    "n_steps = model.n_steps   \n",
    "total_ts = 500 * n_steps * n_envs\n",
    "\n",
    "print(\"Starting learning...\")\n",
    "# This can take around 10 minutes on a Mac laptop\n",
    "model.learn(total_ts, log_interval=10)\n",
    "print(\"Learning finished.\")\n",
    "model.save(save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "save = \"ckpt/half_cheetah_ppo\"\n",
    "\n",
    "# Load and test saved model\n",
    "import time\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode=\"human\")\n",
    "env.reset()\n",
    "# env = gym.make(\"racetrack-fast-v0\", render_mode=\"rgb_array\")\n",
    "model = PPO.load(save)\n",
    "\n",
    "while True:\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "  # env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
